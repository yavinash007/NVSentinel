# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

load('ext://helm_resource', 'helm_resource', 'helm_repo')
load('ext://namespace', 'namespace_create', 'namespace_inject')
load('./helpers.star', 'merge_dicts')

# Suppress unused image warning for log-collector because it's used in dynamically-created
# jobs by the fault-remediation controller (not deployed via Helm templates).
update_settings(
    k8s_upsert_timeout_secs=600,
    suppress_unused_image_warnings=['localhost:5001/ghcr.io_nvidia_nvsentinel_log-collector:latest']
)

num_gpu_nodes = int(os.getenv('NUM_GPU_NODES', '50'))
num_kata_test_nodes = int(os.getenv('NUM_KATA_TEST_NODES', '5'))
use_postgresql = os.getenv('USE_POSTGRESQL', '0') == '1'
user_values_file = os.getenv('NVSENTINEL_VALUES_FILE', '')
print_values = os.getenv('PRINT_HELM_VALUES', '0') == '1'

if use_postgresql:
    print("Using PostgreSQL as datastore (USE_POSTGRESQL=1)")
else:
    print("Using MongoDB as datastore (default). Set USE_POSTGRESQL=1 to use PostgreSQL.")

helm_repo('jetstack', 'https://charts.jetstack.io')
helm_resource(
    'cert-manager',
    chart='jetstack/cert-manager',
    namespace='cert-manager',
    flags=[
        '--create-namespace',
        '--set=installCRDs=true',
    ],
)

helm_repo('prometheus-community', 'https://prometheus-community.github.io/helm-charts')
helm_resource(
    'prometheus-operator',
    chart='prometheus-community/kube-prometheus-stack',
    namespace='monitoring',
    flags=[
        '--create-namespace',
        '--set=prometheus.enabled=true',
        '--set=alertmanager.enabled=false',
        '--set=grafana.enabled=false',
        '--set=kubeStateMetrics.enabled=false',
        '--set=nodeExporter.enabled=false',
        '--set=prometheusOperator.enabled=true',
    ],
)

helm_repo('sigs-kwok', 'https://kwok.sigs.k8s.io/charts/')
helm_resource(
    'kwok',
    chart='sigs-kwok/kwok',
    namespace='kube-system',
    flags=[
        '--set=hostNetwork=true'
    ]
)
helm_resource(
    'kwok-stage-fast',
    chart='sigs-kwok/stage-fast',
    resource_deps=['kwok'],
    pod_readiness='ignore'
)

namespace_create('gpu-operator')
namespace_create('nvsentinel')

# Detect architecture early for KWOK node creation
arch = str(local('uname -m')).strip()
# Normalize architecture names for Kubernetes
if arch in ['arm64', 'aarch64']:
    k8s_arch = 'arm64'
elif arch in ['x86_64', 'amd64']:
    k8s_arch = 'amd64'
else:
    k8s_arch = 'amd64'  # Default to amd64 for unknown architectures

skip_kwok_nodes = os.getenv('SKIP_KWOK_NODES_IN_TILT', '0') == '1'

if not skip_kwok_nodes:
    # Create regular GPU nodes (all NUM_GPU_NODES are regular nodes)
    kwok_node_template = str(read_file('./kwok-node-template.yaml'))
    for i in range(num_gpu_nodes):
        node_yaml = kwok_node_template.replace('PLACEHOLDER', str(i)).replace('amd64', k8s_arch)
        k8s_yaml(blob(node_yaml))

    # Create separate Kata test nodes (in addition to regular nodes)
    # These are named differently (kwok-kata-test-node-*) to avoid conflicts
    # Set NUM_KATA_TEST_NODES=0 to disable kata testing
    kwok_kata_test_node_template = str(read_file('./kwok-kata-test-node-template.yaml'))
    for i in range(num_kata_test_nodes):
        node_yaml = kwok_kata_test_node_template.replace('PLACEHOLDER', str(i)).replace('amd64', k8s_arch)
        k8s_yaml(blob(node_yaml))
else:
    print("Skipping KWOK node creation in Tilt (SKIP_KWOK_NODES_IN_TILT=1)")

k8s_yaml('./nvidia-driver-daemonset.yaml')
k8s_yaml('./nvidia-dcgm-daemonset.yaml')

include('../fault-quarantine/Tiltfile')
include('../fault-remediation/Tiltfile')
include('../janitor/Tiltfile')
include('../janitor-provider/Tiltfile')
include('../node-drainer/Tiltfile')
include('../platform-connectors/Tiltfile')
include('./simple-health-client/Tiltfile')
include('./event-exporter-mock/Tiltfile')
include('./csp-api-mock/Tiltfile')
include('../health-events-analyzer/Tiltfile')
include('../event-exporter/Tiltfile')
include('../health-monitors/gpu-health-monitor/Tiltfile')
include('../health-monitors/csp-health-monitor/Tiltfile')
include('../health-monitors/kubernetes-object-monitor/Tiltfile')
include('../health-monitors/syslog-health-monitor/Tiltfile')
include('../labeler/Tiltfile')
include('../log-collector/Tiltfile')
include('../gpu-reset/Tiltfile')
values_files = [
    '../distros/kubernetes/nvsentinel/values.yaml',
    '../distros/kubernetes/nvsentinel/values-tilt.yaml',
    '../distros/kubernetes/nvsentinel/values-tilt-socket.yaml'  # Ensures /var/run exists for socket
]

# Add datastore-specific values
if use_postgresql:
    values_files.append('../distros/kubernetes/nvsentinel/values-tilt-postgresql.yaml')
else:
    values_files.append('../distros/kubernetes/nvsentinel/values-tilt-mongodb.yaml')

# Add ARM64-specific values if on ARM64 (arch already detected earlier)
if k8s_arch == 'arm64':
    values_files.append('../distros/kubernetes/nvsentinel/values-tilt-arm64.yaml')

# Add user-specified values file if provided. It should be the last in the chain to override other settings.
if user_values_file:
    if os.path.exists(user_values_file):
        print("Using user-specified values file: " + user_values_file)
        values_files.append(user_values_file)
    else:
        warn("User-specified values file not found, ignoring: " + user_values_file)

effective_values = {}
for values_file in values_files:
    file_values = read_yaml(values_file)
    effective_values = merge_dicts(effective_values, file_values)

if print_values:
    print("Effective Helm values for NVSentinel:")
    print("---")
    print(encode_yaml(effective_values))
    print("---")

yaml = helm(
    '../distros/kubernetes/nvsentinel',
    name='nvsentinel',
    namespace='nvsentinel',
    values=values_files,
)

local_resource(
    'wait-for-cert-manager-crds',
    cmd='''
        echo "Waiting for cert-manager webhook to be ready..."
        kubectl rollout status deployment/cert-manager-webhook -n cert-manager --timeout=300s

        echo "Verifying CRDs..."
        kubectl wait --for=condition=established --timeout=60s crd/certificates.cert-manager.io crd/issuers.cert-manager.io

        echo "cert-manager is ready"
    ''',
    resource_deps=['cert-manager'],
)

k8s_yaml(yaml)

# Check if using Percona operator (only relevant when using MongoDB)
use_percona = effective_values.get('mongodb-store', {}).get('usePerconaOperator', False) if not use_postgresql else False
if use_percona:
    print("Using Percona MongoDB Operator for MongoDB installation.")
else:
    print("Using bitnami MongoDB installation.")

# Determine datastore resource name
if use_postgresql:
    datastore_resource = 'nvsentinel-postgresql'
else:
    datastore_resource = 'mongodb' if not use_percona else 'create-mongodb-database'

# Cert-manager resources vary based on datastore
cert_manager_objects = [
    'janitor-webhook-cert:certificate',
    'janitor-selfsigned-issuer:issuer'
]
if use_postgresql:
    cert_manager_objects.extend([
        'postgresql-root-ca:certificate',
        'postgresql-ca-issuer:issuer',
        'selfsigned-ca-issuer:issuer',
        'postgresql-server-cert:certificate',
        'postgresql-client-cert:certificate'
    ])
else:
    if use_percona:
        cert_manager_objects.append('mongo-app-client-cert:certificate')
    else:
        cert_manager_objects.extend([
            'mongo-root-ca:certificate',
            'mongo-ca-issuer:issuer',
            'selfsigned-ca-issuer:issuer',
            'mongo-server-cert-0:certificate',
            'mongo-app-client-cert:certificate'
        ])

k8s_resource(
    new_name='cert-manager-resources',
    objects=cert_manager_objects,
    resource_deps=['wait-for-cert-manager-crds'],
)

local_resource(
    'wait-for-janitor-cert',
    cmd='kubectl wait --for=condition=Ready --timeout=300s certificate/janitor-webhook-cert -n nvsentinel',
    resource_deps=['cert-manager-resources'],
)

k8s_resource(
    'janitor',
    resource_deps=['wait-for-janitor-cert'],
)

k8s_resource(
    'janitor-provider',
    resource_deps=['janitor'],
)

if use_percona:
    k8s_resource(
        'nvsentinel-psmdb-operator',
        resource_deps=['wait-for-cert-manager-crds'],
    )

# MongoDB database creation (only for MongoDB, not PostgreSQL)
if not use_postgresql:
    k8s_resource(
        'create-mongodb-database',
        resource_deps=['wait-for-cert-manager-crds', 'nvsentinel-psmdb-operator'] if use_percona else ['wait-for-cert-manager-crds'],
    )

    if not use_percona:
        k8s_resource(
            'mongodb',
            resource_deps=['wait-for-cert-manager-crds'],
        )

k8s_resource(
    new_name='prometheus-resources',
    objects=['nvsentinel:podmonitor'],
    resource_deps=['prometheus-operator'],
)
k8s_resource(
    'prometheus-operator',
    port_forwards='9090:9090',
)

# Configure datastore StatefulSet with optional port forwarding
if use_postgresql:
    k8s_resource(
        workload='nvsentinel-postgresql',
        port_forwards=['5432:5432'],  # Port-forward PostgreSQL for debugging
        resource_deps=['cert-manager-resources'],
    )

# Group all KWOK nodes (regular GPU nodes + separate kata test nodes) for resource management
if not skip_kwok_nodes:
    kwok_regular_node_names = ['kwok-node-' + str(i) + ':node' for i in range(num_gpu_nodes)]
    kwok_kata_test_node_names = ['kwok-kata-test-node-' + str(i) + ':node' for i in range(num_kata_test_nodes)]
    kwok_all_node_names = kwok_regular_node_names + kwok_kata_test_node_names

    k8s_resource(
        new_name='kwok-fake-nodes',
        objects=kwok_all_node_names,
        resource_deps=['kwok', 'platform-connectors', 'fault-quarantine', 'fault-remediation',
            'labeler', 'node-drainer', datastore_resource, 'simple-health-client'
        ],
    )

k8s_resource(
    'gpu-health-monitor-dcgm-4.x',
    resource_deps=['platform-connectors']
)

k8s_resource(
    'kubernetes-object-monitor',
    resource_deps=['platform-connectors']
)

k8s_resource(
    'syslog-health-monitor-regular',
    resource_deps=['platform-connectors']
)

k8s_resource(
    'simple-health-client',
    resource_deps=['platform-connectors']
)

k8s_resource(
    'event-exporter',
    port_forwards='2112:2112',
    labels=['event-exporter'],
    resource_deps=['event-exporter-mock', datastore_resource]
)

k8s_resource(
    'platform-connectors',
    resource_deps=[datastore_resource]
)

k8s_resource(
    'fault-quarantine',
    resource_deps=[datastore_resource]
)

k8s_resource(
    'node-drainer',
    resource_deps=[datastore_resource]
)

k8s_resource(
    'fault-remediation',
    resource_deps=[datastore_resource]
)

k8s_resource(
    'csp-health-monitor',
    resource_deps=[datastore_resource, 'csp-api-mock']
)

k8s_resource(
    'kwok-stage-fast',
    pod_readiness='ignore',
    resource_deps=['kwok']
)
k8s_resource(
    workload='gpu-health-monitor-dcgm-3.x',
    pod_readiness='ignore'
)
k8s_resource(
    workload='syslog-health-monitor-kata',
    pod_readiness='ignore'
)

# This is a workaround to ensure that pods are deleted with respect to finalizers
# source: https://github.com/kubernetes-sigs/kwok/discussions/926#discussioncomment-8204558
k8s_yaml('./kwok-pod-delete-respect-finalizers.yaml')
k8s_resource(
    new_name='kwok-pod-delete-stage',
    objects=['pod-delete:stage'],
    resource_deps=['kwok', 'kwok-stage-fast']
)
